#!/usr/bin/env python3

"""
A utility function for quickly looking up log files using Databrick's databricks cli

Run `dbfs-logs-search --help` for more information
"""

import keyring
import subprocess
import os
import re
import argparse
import pandas as pd
import numpy as np
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
import time


def dbfs_log_search(
    cluster_pattern: chr = ".",
    file_pattern: chr = ".",
    type: list = None,
    cache: bool = True,
    n: int = 1,
    force: bool = False,
    cat: bool = True,
    host: chr = None,
    token: chr = None,
    verbose: bool = True,
) -> None:
    dbfs_configure(token=token, host=host)

    # retrieve cluster information
    all_clusters = get_clusters(
        cache=cache,
        force=force,
        verbose=verbose,
    )

    clusters = [
        cluster for cluster in all_clusters if re.search(cluster_pattern, cluster)
    ]

    n_clusters = len(clusters)
    all_clusters = "\n".join(all_clusters)

    if n_clusters == 0:
        raise Exception(
            f"No clusters found with pattern '{cluster_pattern}', "
            "expected one of:\n{all_clusters}"
        )

    if n_clusters > 1:
        raise Exception(f"Found {n} clusters, expected one of:\n{all_clusters}")

    cluster = clusters[0]

    # retrieve logs information
    df = get_logs_info(
        cluster=cluster,
        cache=cache,
        force=force,
        verbose=verbose,
    )

    ## get time offset
    offset = time.timezone
    ## convert to local time
    df["localtime"] = df["datetime"] - pd.Timedelta(seconds=offset)

    ## filter by file regex
    df = df[df["script"].str.contains(file_pattern)]

    if type is None:
        type = ["stdout", "stderr"]
    elif isinstance(type, str):
        type = [type]

    if n == 0:
        n = 1

    n = n * len(type)
    assert [t in ["stdout", "stderr"] for t in type]
    df = df[df["type"].isin(type)]

    ## unique by datetime, script, type
    df = df.drop_duplicates(["datetime", "script", "type"])
    df = df.sort_values("datetime", ascending=False)
    df = df.reset_index(drop=True)
    if verbose:
        print(f"Found {len(df)} files")

    # print outputs
    if len(df) == 0:
        raise Exception(f"No files found with pattern {file_pattern}")

    if n > len(df):
        if verbose:
            print(f"Showing all {len(df)} files")
        n = len(df)
    else:
        if verbose:
            print(f"Showing first {n} files")
        df = df[0:n]

    if verbose or not cat:
        # add extra whitespace to path to make characters the same
        print_df = df.copy()
        print_df["path"] = df["path"].str.ljust(np.max(df["path"].str.len()) + 1)
        print(print_df[["localtime", "path"]].to_string(index=False, justify="left"))

    if cat:
        for i in range(n):
            # print with header ====
            print("\n" + "".ljust(80, "="))
            # print out information
            print(f"datetime  : {df['datetime'][i]} UTC")
            print(f"localtime : {df['localtime'][i]} local")
            print(f"script    : {df['script'][i]}")
            print(f"type      : {df['type'][i]}")
            print(f"path      : {df['path'][i]}")
            print("".ljust(80, "=") + "\n")
            run_stream("databricks", "fs", "cat", df["path"][i])

    return None


def get_clusters(
    cache: bool = True,
    force: bool = False,
    verbose: bool = True,
) -> list:
    path = cache_path("clusters.json")

    try:
        # get mtime as UTC
        mtime = os.path.getmtime(path)
    except FileNotFoundError:
        mtime = 0

    # get now as UTC
    now = pd.Timestamp.utcnow()

    if (
        cache
        and not force
        and os.path.exists(path)
        and (mtime > (now - pd.Timedelta(minutes=10)).timestamp())
    ):
        ts = pd.Timestamp.fromtimestamp(mtime)
        expiration = 600 - (now.timestamp() - mtime)
        expiration = int(expiration)

        if verbose:
            print(f"Found cached  : {path}")
            print(f"Last modified : {ts}")
            print(f"Expires in    : {expiration} seconds")

        clusters = pd.read_json(path)["clusters"]
    else:
        clusters = run_output("databricks", "fs", "ls", "dbfs:/cluster-logs")
        # save clusters
        pd.DataFrame({"clusters": clusters}).to_json(path)

    return clusters


def get_logs_info(
    cluster: chr,
    cache: bool = True,
    force: bool = False,
    verbose: bool = True,
) -> pd.DataFrame:
    path = cache_path(cluster + ".pkl")

    try:
        # get mtime as UTC
        mtime = os.path.getmtime(path)
    except FileNotFoundError:
        mtime = 0

    # get now as UTC
    now = pd.Timestamp.utcnow()

    if (
        cache
        and not force
        and os.path.exists(path)
        and (mtime > (now - pd.Timedelta(minutes=10)).timestamp())
    ):
        ts = pd.Timestamp.fromtimestamp(mtime)
        expiration = 600 - (now.timestamp() - mtime)
        expiration = int(expiration)

        if verbose:
            print(f"Found cached  : {path}")
            print(f"Last modified : {ts}")
            print(f"Expires in    : {expiration} seconds")

        df = pd.read_pickle(path)
        # early exit
        return df

    if verbose:
        print(f"Cache not found or too old: {path}")
        print("Searching for logs...")

    info = {}
    dirs = run_output(
        "databricks",
        "fs",
        "ls",
        "--absolute",
        f"dbfs:/cluster-logs/{cluster}/init_scripts",
    )

    n_dirs = len(dirs)

    # perform asynchronously
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(get_info, dir): dir for dir in dirs}
        for future in as_completed(futures):
            dir = futures[future]
            info[dir] = future.result()
            if verbose:
                print(f"Searching {len(info)}/{n_dirs} directories...", end="\r")
    if verbose:
        print("")

    df = pd.concat([pd.DataFrame(info[dir]) for dir in info], ignore_index=True)
    df["datetime"] = pd.to_datetime(df["datetime"], format="%Y%m%d_%H%M%S")

    # convert each item into a DataFrame, then bind together
    df = pd.concat([pd.DataFrame(info[dir]) for dir in info], ignore_index=True)

    if cache:
        # store in cache
        pd.to_pickle(df, path)

    return df


def cache_path(*args) -> chr:
    cache_dir = os.path.join(os.path.expanduser("~"), ".dbfs-logs-search")
    os.makedirs(cache_dir, exist_ok=True)

    # when *args is none, just return cache_dir
    if len(args) == 0:
        return cache_dir

    path = os.path.join(cache_dir, *args)
    return path


def get_info(dir) -> dict:
    # path: 20230725_190947_00_update_system_packages.sh.stderr.log
    # starts with date time
    # ends with script name
    files = run_output("databricks", "fs", "ls", dir)
    # extract datetime from beginning of path, first 15 characters
    datetimes = [file[0:15] for file in files]
    datetimes = [
        pd.to_datetime(datetime, format="%Y%m%d_%H%M%S") for datetime in datetimes
    ]
    # extract script from end of path: after 18th character, with last 14
    # characters removed
    scripts = [file[19:-14] for file in files]
    # extract output put
    types = [file[-10:-4] for file in files]
    res = {
        "directory": [dir] * len(files),
        "datetime": datetimes,
        "script": scripts,
        "type": types,
        "path": [dir + "/" + file for file in files],
    }
    return res


def run_output(*args):
    res = subprocess.run(list(args), check=False, stdout=subprocess.PIPE)
    out = res.stdout.decode("utf-8").rsplit()
    return out


def run_stream(*args):
    try:
        subprocess.run(
            list(args),
            check=True,
            stdout=sys.stderr,
            stderr=sys.stderr,
            text=True,
            encoding="utf-8",
        )
    except subprocess.CalledProcessError:
        print(f"Error running {' '.join(args)}")
        pass

    return None


def read_config() -> None:
    """
    The databricks cli I think is messing up the toml and not using "'" for
    strings, so I'm going to have to read it manually
    """
    path = os.path.expanduser("~/.databrickscfg")

    if not path:
        return None

    with open(path, "r") as f:
        lines = f.readlines()

    # find "host = " and "token = "
    host = None
    token = None

    for line in lines:
        if line.startswith("host"):
            host = line.split("=")[1].strip()
        elif line.startswith("token"):
            token = line.split("=")[1].strip()

    return {"DEFAULT": {"host": host, "token": token}}


def dbfs_configure(host: chr = None, token: chr = None) -> None:
    cfg = read_config()

    if host is None:
        if cfg is None:
            host = os.getenv("DATABRICKS_HOST", None)
        else:
            host = cfg["DEFAULT"]["host"]

        if host is None:
            raise Exception(
                "No host found, please provide a host or set "
                "DATABRICKS_HOST environment variable"
            )

    if not host.startswith("https:/"):
        host = "https://" + host

    if token is None:
        if cfg is None:
            token = os.getenv("DATABRICKS_TOKEN", None)
        else:
            token = cfg["DEFAULT"]["token"]

        if token is None:
            token = keyring.get_password("databricks", host)
            if token is None:
                raise Exception(
                    f"No token found with host {host}.  Please "
                    "provide a token or set DATABRICKS_TOKEN "
                    "environment variable"
                )

    # https://stackoverflow.com/a/55081210/12126576

    # write to file
    with open("/tmp/databrickscfg", "w") as f:
        f.write(f"[DEFAULT]\nhost = {host}\ntoken = {token}\n")

    return None


def cache_clear():
    # find total size of cache
    total_size = 0
    for root, _, files in os.walk(cache_path()):
        for file in files:
            path = os.path.join(root, file)
            total_size += os.path.getsize(path)

    print(f"Total size of cache: {total_size} bytes")
    # delete cache directory
    os.system(f"rm -rf {cache_path()}")
    return None


def check_dbfs():
    try:
        subprocess.run(["databricks", "--version"], check=True)
    except FileNotFoundError:
        raise Exception("databricks cli not found, please install it")
    return None


EXAMPLE_TEXT = """
examples:
    $ dbfs-logs-search npe r_pkgs
    $ dbfs-logs-search npe r_pkgs --no-cat
    $ dbfs-logs-search npe r_pkgs stderr
    $ dbfs-logs-search --cache-clear
    $ dbfs-logs-search --get-clusters
    $ dbfs-logs-search --get-cache-path
"""


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Tries to print out the most recent log",
        epilog=EXAMPLE_TEXT,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "cluster_pattern",
        help="A pattern to match for the log directory",
        nargs="?",
        default=".",
    )
    parser.add_argument(
        "file_pattern",
        help="A pattern to match for the log file",
        nargs="?",
        default=".",
    )
    parser.add_argument(
        "type",
        help="The type of log file to search for",
        nargs="?",
        choices=["stdout", "stderr"],
    )
    parser.add_argument(
        "-n",
        help="Number of files to print; for --get-logs-info, number of rows to print",
        type=int,
        default=0,
    )
    parser.add_argument(
        "--cache",
        help="Use cached data",
        action=argparse.BooleanOptionalAction,
        default=True,
    )
    parser.add_argument(
        "--force",
        "-f",
        help="Force cache refresh",
        action=argparse.BooleanOptionalAction,
        default=False,
    )
    parser.add_argument(
        "-c",
        "--cat",
        help="Controls printing for files",
        action=argparse.BooleanOptionalAction,
        default=True,
    )
    parser.add_argument(
        "--host",
        help="Host name (optional)",
        required=False,
    )
    parser.add_argument(
        "--token",
        help="Include a token (optional)",
        required=False,
    )
    parser.add_argument(
        "--verbose",
        "-v",
        help="Verbose output (ignored with immediate exits, which are not verbose)",
        action=argparse.BooleanOptionalAction,
        default=True,
    )
    parser.add_argument(
        "--cache-clear",
        help="Clear the cache and exit",
        action=argparse.BooleanOptionalAction,
        default=False,
    )
    parser.add_argument(
        "--get-clusters",
        help="Get clusters and exit",
        action=argparse.BooleanOptionalAction,
        default=False,
    )
    parser.add_argument(
        "--get-cache-path",
        help="Get cache path and exit",
        action=argparse.BooleanOptionalAction,
        default=False,
    )
    parser.add_argument(
        "--get-logs-info",
        help="Get logs info and exit",
        action=argparse.BooleanOptionalAction,
        default=False,
    )
    parser.add_argument(
        "--get-logs-path",
        help="Get logs paths and exit",
        action=argparse.BooleanOptionalAction,
        default=False,
    )
    args = parser.parse_args()

    if args.cache_clear:
        print("Clearing cache...")
        cache_clear()
        exit(0)

    if args.get_clusters:
        clusters = get_clusters(
            cache=args.cache,
            force=args.force,
            verbose=False,
        )
        print("\n".join(clusters))
        exit(0)

    if args.get_cache_path:
        path = cache_path()
        print(path)
        exit(0)

    if args.get_logs_info or args.get_logs_path:
        clusters = get_clusters(
            cache=args.cache,
            force=args.force,
            verbose=False,
        )
        # match arg.cluster
        cluster = [
            cluster for cluster in clusters if re.search(args.cluster_pattern, cluster)
        ][0]
        df = get_logs_info(
            cluster=cluster,
            cache=args.cache,
            force=args.force,
            verbose=False,
        )
        # arrange by time and path
        df = df.sort_values(["datetime", "path"], ascending=False)
        # filter by file regex
        df = df[df["script"].str.contains(args.file_pattern)]

        # filter by type
        if args.type is None:
            type = ["stdout", "stderr"]
        elif isinstance(args.type, str):
            type = [args.type]
        else:
            type = args.type

        assert [t in ["stdout", "stderr"] for t in type]
        # filter on type
        df = df[df["type"].isin(type)]

        # filter by n
        if args.n > 0:
            df = df[0 : args.n]

        if args.get_logs_info:
            # convert all columns to string
            df = df.applymap(str)
            # left align all columns
            df = df.apply(lambda x: x.str.ljust(x.str.len().max()), axis=0)
            print(df.to_string(justify="left", index=False))
        elif args.get_logs_path:
            paths = df["path"].tolist()
            print("\n".join(paths))

        exit(0)

    dbfs_log_search(
        cluster_pattern=args.cluster_pattern,
        file_pattern=args.file_pattern,
        type=args.type,
        n=args.n,
        cache=args.cache,
        force=args.force,
        cat=args.cat,
        host=args.host,
        token=args.token,
        verbose=args.verbose,
    )
