#!/usr/bin/env python3

"""
A utility function for quickly looking up log files using Databrick's dbfs cli
"""


import keyring
import subprocess
import tempfile
import os
import re
import argparse
import pandas as pd
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
import time


def dbfs_log_search(
        cluster_pattern: chr = ".",
        file_pattern: chr = ".",
        cache : bool = True,
        n : int = 2,
        force : bool = False,
        cat : bool = True,
        host: chr = None,
        token: chr = None
) -> None :
    dbfs_configure(token=token, host=host)
    cache_dir = os.path.join(os.path.expanduser("~"), ".dbfs-logs-search")
    os.makedirs(cache_dir, exist_ok=True)

    cache_path = os.path.join(cache_dir, "clusters.json")
    try :
        cache_path_time = os.path.getmtime(cache_path)
    except FileNotFoundError :
        cache_path_time = 0
    if (
        cache
        and not force
        and os.path.exists(cache_path)
        and (cache_path_time > (pd.Timestamp.now() - pd.Timedelta(minutes=10)).timestamp())
    ) :
        cache_timestamp = pd.Timestamp.fromtimestamp(cache_path_time)
        expiration = (pd.Timestamp.now() - cache_timestamp).seconds

        print(f"Found cached config: {cache_path}")
        print(f"Last modified: {cache_timestamp}")
        print(f"Expires in : {expiration} seconds")
        clusters = pd.read_json(cache_path)["clusters"]
    else :
        clusters = run_output("dbfs", "ls", "dbfs:/cluster-logs")
        # save clusters
        pd.DataFrame({"clusters" : clusters}).to_json(cache_path)

    # print clusters on each line
    print(f"Clusters [{len(clusters)}]:")
    print("\n".join(clusters))
    clusters = [cluster for cluster in clusters if re.search(cluster_pattern, cluster)]
    n_clusters = len(clusters)

    if n_clusters == 0 :
        raise Exception(f"No clusters found with pattern {cluster_pattern}")

    if n_clusters > 1 :
        raise Exception(f"Found {n} clusters, expected 1: {clusters}")

    cluster = clusters[0]
    cache_path = os.path.join(cache_dir, cluster + ".pkl")
    try :
        cache_path_time = os.path.getmtime(cache_path)
    except FileNotFoundError :
        cache_path_time = 0
    if ( 
        cache 
        and not force
        and os.path.exists(cache_path) 
        and (cache_path_time > (pd.Timestamp.now() - pd.Timedelta(minutes=10)).timestamp())
     ) :
        cache_timestamp = pd.Timestamp.fromtimestamp(cache_path_time)
        expiration = (pd.Timestamp.now() - cache_timestamp).seconds

        print(f"Found cached config: {cache_path}")
        print(f"Last modified: {cache_timestamp}")
        print(f"Expires in : {expiration} seconds")
        df = pd.read_pickle(cache_path)
        
    else :
        print(f"Cache not found or too old: {cache_path}") 
        print("Searching for logs...")
        info = {}
        dirs = run_output("dbfs", "ls", "--absolute", f"dbfs:/cluster-logs/{cluster}/init_scripts")
        n_dirs = len(dirs)

        # perform asynchronously
        with ThreadPoolExecutor(max_workers=10) as executor :
            futures = {executor.submit(get_info, dir) : dir for dir in dirs}
            for future in as_completed(futures) :
                dir = futures[future]
                info[dir] = future.result()
                print(f"Searching {len(info)}/{n_dirs} directories...", end="\r")

        
        df = pd.concat([pd.DataFrame(info[dir]) for dir in info], ignore_index=True)
        df["datetime"] = pd.to_datetime(df["datetime"], format="%Y%m%d_%H%M%S")

        # convert each item into a DataFrame, then bind together
        df = pd.concat([pd.DataFrame(info[dir]) for dir in info], ignore_index=True)

        if cache :
            # store in cache
            pd.to_pickle(df, cache_path)

    # get time offset
    offset = time.altzone
    # convert to local time
    df["datetime"] = df["datetime"] - pd.Timedelta(seconds=offset)
    
    # filter by file_pattern
    df = df[df["script"].str.contains(file_pattern)]
    df = df.sort_values("datetime", ascending=False)
    df = df.reset_index(drop=True)
    print(f"Found {len(df)} files")

    if len(df) == 0 :
        raise Exception(f"No files found with pattern {file_pattern}")

    if n > len(df) :
        print(f"Showing all {len(df)} files")
        n = len(df)
    else :
        print(f"Showing first {n} files")
        df = df[0:n]

    if cat :
        for i in range(n) :
            # print with header ====
            print("\n" + "".ljust(80, "="))
            print(f"{df['path'][i]}")
            print(f"{df['datetime'][i]}")
            print("".ljust(80, "=") + "\n")
            run_stream("dbfs", "cat", df["path"][i])
    else :
        # print datetime and paths without index
        print(df[["datetime", "path"]].to_string(index=False))

    return None


def get_info(dir) -> dict :
    # path: 20230725_190947_00_update_system_packages.sh.stderr.log
    # starts with date time
    # ends with script name
    files = run_output("dbfs", "ls", dir)
    # extract datetime from beginning of path, first 15 characters
    datetimes = [file[0:15] for file in files]
    datetimes = [pd.to_datetime(datetime, format="%Y%m%d_%H%M%S") for datetime in datetimes]
    # extract script from end of path: after 18th character, with last 14 characters removed
    scripts = [file[19:-14] for file in files]
    res = {
        "directory" : [dir] * len(files),
        "datetime" : datetimes,
        "script" : scripts,
        "path" : [dir + "/" + file for file in files],
    }
    return res


def run_output(*args) :
    res = subprocess.run(list(args), check=False, stdout=subprocess.PIPE)
    out = res.stdout.decode("utf-8").rsplit()
    return out


def run_stream(*args) :
    try :
        subprocess.run(
            list(args),
            check=True,
            stdout=sys.stderr,
            stderr=sys.stderr,
            text=True,
            encoding="utf-8"
        )
    except subprocess.CalledProcessError :
        print(f"Error running {' '.join(args)}")
        pass
    
    return None


def dbfs_configure(
        host: chr = None,
        token: chr = None
) -> None :
    if token is None :
        return None
    
    token = keyring.get_password("dbfs", host)
    if token is None :
        raise Exception(f"No token found with host {host}")

    if not host.startswith("https:/") :
        host = "https://" + host

    # https://stackoverflow.com/a/55081210/12126576
    temp = tempfile.NamedTemporaryFile(mode="w+t", delete=False)
    with temp as f :
        f.writelines(token)

    args = ["dbfs", "configure", "--host", host, "--token-file", temp.name]
    subprocess.run(args, check=True)
    os.unlink(temp.name)
    return None


def cache_clear() :
    os.unlink(os.path.join(os.path.expanduser("~"), ".dbfs-logs-search-clusters"))
    return None

EXAMPLE_TEXT = """
examples:

    $ dbfs-logs-search npe r_pkgs
    $ dbfs-logs-search npe r_pkgs --single
    $ dbfs-logs-search npe r_pkgs --no-cat
"""


if __name__ == "__main__" :
    parser = argparse.ArgumentParser(
        description="Tries to print out the most recent log",
        epilog=EXAMPLE_TEXT,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "cluster_pattern",
        help="A pattern to match for the log directory",
        nargs="?",
        default="."
    )
    parser.add_argument(
        "file_pattern",
        help="A pattern to match for the log file",
        nargs="?",
        default="."
    )
    parser.add_argument(
        "-n",
        help="Number of files to print",
        type=int,
        default=2
    )
    parser.add_argument(
        "--cache",
        help="Use cached data",
        action=argparse.BooleanOptionalAction,
        default=True
    )
    parser.add_argument(
        "--force", "-f",
        help="Force cache refresh",
        action=argparse.BooleanOptionalAction,
        default=False
    )
    parser.add_argument(
        "-c", "--cat",
        help="Controls printing for files",
        action=argparse.BooleanOptionalAction,
        default=True
    )
    parser.add_argument(
        "--host",
        help="Host name (optional)",
        required=False
    )
    parser.add_argument(
        "--token",
        help="Include a token (optional)",
        required=False
    )
    parser.add_argument(
        "--cache-clear",
        help="Clear the cache and exit",
        action=argparse.BooleanOptionalAction,
        default=False
    )
    args = parser.parse_args()

    if args.cache_clear :
        print("Clearing cache...")
        cache_clear()
        exit(0)

    dbfs_log_search(
        cluster_pattern=args.cluster_pattern,
        file_pattern=args.file_pattern,
        n=args.n,
        cache=args.cache,
        force=args.force,
        cat=args.cat,
        host=args.host,
        token=args.token
    )
